{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceea56db-8e50-482c-88c5-9f279c1cb365",
   "metadata": {},
   "source": [
    "## IMPORT STATEMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d80d6f40-9749-44f1-978b-6f0c0b9590e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imbris/anaconda3/envs/lamda8/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colossalai should be built with cuda extension to use the FP16 optimizer\n",
      "If you want to activate cuda mode for MoE, please install with cuda_ext!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import wandb\n",
    "import einops\n",
    "import datasets\n",
    "import itertools\n",
    "from transformers import AutoTokenizer\n",
    "#===============+ PyTorch\n",
    "import torch\n",
    "from lamda_pytorch.config.config import CFG\n",
    "from lamda_pytorch.build_dataloader import build_dataloaders\n",
    "from lamda_pytorch.lamda_pytorch import lamda_model\n",
    "from lamda_pytorch.utils.utils import LaMDA_Loss, AutoregressiveWrapper\n",
    "#===============+ Colossal AI\n",
    "import colossalai\n",
    "from colossalai.core import global_context as gpc\n",
    "from colossalai.trainer import Trainer, hooks\n",
    "from colossalai.utils import MultiTimer\n",
    "from colossalai.logging import disable_existing_loggers, get_dist_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09d9847e-662e-4b12-a6dc-a5eb3118f296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cacf642-e019-449b-831b-7a4157d73fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LaMDA_Trainer(cfg: CFG):\n",
    "    assert torch.cuda.is_available()\n",
    "    disable_existing_loggers()\n",
    "\n",
    "    parser = colossalai.get_default_parser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--use_trainer',\n",
    "        action='store_true',\n",
    "        help='whether to use trainer'\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if cfg.use_zero == True:\n",
    "        pass\n",
    "    else:\n",
    "        colossalai.launch_from_torch(\n",
    "            config='./lamda_pytorch/config/colossal_config.py', \n",
    "            seed = cfg.seed\n",
    "        )\n",
    "\n",
    "    assert hasattr(gpc.config, \"EPOCHS\"), \"Please provide NUM_EPOCHS in your configuration\"\n",
    "\n",
    "    # Colossal logger\n",
    "    logger = get_dist_logger()\n",
    "    logger.info(\"Initialized environment\", ranks=[0])\n",
    "\n",
    "    # LaMDA model\n",
    "    model = lamda_model()\n",
    "    model = AutoregressiveWrapper(model)\n",
    "\n",
    "    # setup dataloaders\n",
    "    if cfg.use_huggingface == True:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(cfg.tokenizer_name)\n",
    "        train_dataloader, eval_dataloader = build_dataloaders(cfg, tokenizer)\n",
    "\n",
    "    # loss function\n",
    "    loss_fn = LaMDA_Loss()\n",
    "\n",
    "    # optimizer function\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr = gpc.config.LEARNING_RATE,\n",
    "        weight_decay=gpc.config.WEIGHT_DECAY\n",
    "    )\n",
    "\n",
    "    # initialze model, optimizer, criterion, and data loaders\n",
    "\n",
    "    engine, train_dataloader, _, _ = colossalai.initialize(\n",
    "        model,\n",
    "        optimizer,\n",
    "        loss_fn,\n",
    "        train_dataloader = train_dataloader\n",
    "    )\n",
    "\n",
    "    def batch_data_process_func(batch_data):\n",
    "        data = batch_data[\"input_ids\"]\n",
    "        labels = batch_data[\"labels\"]\n",
    "        return data, labels\n",
    "\n",
    "    engine.schedule.batch_data_process_func = batch_data_process_func\n",
    "\n",
    "    if cfg.use_wandb == True:\n",
    "\n",
    "        # initialize Weights and Biases Logging\n",
    "        wandb.init(project = cfg.project_name)\n",
    "\n",
    "        engine.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            inputs, labels = batch['inputs'].cuda(), batch['labels'].cuda()\n",
    "            \n",
    "            engine.zero_grad()\n",
    "            outputs = engine(inputs)\n",
    "\n",
    "            train_loss = engine.loss_fn(outputs, labels)\n",
    "            wandb.log({\"train_loss\": train_loss})\n",
    "\n",
    "            engine.backward(train_loss)\n",
    "            engine.step()\n",
    "            wandb.log({\"step\": step})\n",
    "            \n",
    "            engine.eval()\n",
    "            for step, batch in enumerate(eval_dataloader):\n",
    "                inputs, labels = batch['inputs'].cuda(), batch['labels'].cuda()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = engine(inputs)\n",
    "                    test_loss = engine.loss_fn(outputs, labels)\n",
    "                    wandb.log({\"test_loss\": test_loss})\n",
    "                \n",
    "                engine.backward(test_loss)\n",
    "                engine.step()\n",
    "\n",
    "        wandb.alert(\n",
    "            title = 'Training Complete',\n",
    "            text = \"Training complete.\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Time session with ColossalAI\n",
    "        timer = MultiTimer()\n",
    "\n",
    "        # trainer\n",
    "        trainer = Trainer(\n",
    "            engine = engine,\n",
    "            timer =  timer,\n",
    "            logger = logger\n",
    "        )\n",
    "\n",
    "        hook_list = [\n",
    "            hooks.LogMetricByStepHook(),\n",
    "            hooks.LossHook(),\n",
    "            hooks.LogMetricByEpochHook(logger)\n",
    "        ]\n",
    "\n",
    "        trainer.fit(\n",
    "            train_dataloader = train_dataloader,\n",
    "            epochs = gpc.config.EPOCHS,\n",
    "            hooks = hook_list,\n",
    "            display_progress = True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8f0b88b-223a-4c95-9596-d34d4a9d3f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--config CONFIG] [--host HOST]\n",
      "                             [--port PORT] [--world_size WORLD_SIZE]\n",
      "                             [--rank RANK] [--local_rank LOCAL_RANK]\n",
      "                             [--backend BACKEND] [--use_trainer]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/imbris/.local/share/jupyter/runtime/kernel-a3e0beb9-35e7-42f9-95ba-b83582da83d1.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "cfg = CFG()\n",
    "LaMDA_Trainer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc96fa9-cd7b-47fe-94c2-0b57e50aaf96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
